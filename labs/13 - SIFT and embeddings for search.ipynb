{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Image search using [SIFT](https://www.thepythoncode.com/article/sift-feature-extraction-using-opencv-in-python)\n",
    "\n",
    "Let's think about information retrieval in the context of image search. How can we find images similar to a query in a fast way (faster than doing pair-wise comparison with all images in a database)? How can we identify same objects taken in slightly different contexts? \n",
    "\n",
    "One way to do this is to find special points of interest in every image, so called keypoints (or descriptors), which characterize the image and which are more or less invariant to scaling, orientation, illumination changes, and some other distortions. There are several algorithms available that identify such keypoints, and today we will focus on [SIFT](https://en.wikipedia.org/wiki/Scale-invariant_feature_transform). \n",
    "\n",
    "Your task is to apply SIFT to a dataset of images and enable similar images search.\n",
    "\n",
    "## Get dataset\n",
    "\n",
    "We will use `Caltech 101` dataset, download it from [here](http://www.vision.caltech.edu/Image_Datasets/Caltech101/). It consists of pictures of objects belonging to 101 categories. About 40 to 800 images per category. Most categories have about 50 images. The size of each image is roughly 300 x 200 pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIFT example\n",
    "\n",
    "Below is the example of SIFT keyponts extraction using `opencv`. [This](https://docs.opencv.org/trunk/da/df5/tutorial_py_sift_intro.html) is a dedicated tutorial, and [this](https://docs.opencv.org/master/dc/dc3/tutorial_py_matcher.html) is another tutorial you may need to find matches between two images (use in your code `cv.drawMatches()` function to display keypoint matches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python opencv-contrib-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img_dir = '../../101_ObjectCategories'\n",
    "img = cv.imread(img_dir + '/gramophone/image_0018.jpg')\n",
    "gray= cv.cvtColor(img,cv.COLOR_BGR2GRAY)\n",
    "\n",
    "# older versions of OpenCV\n",
    "# sift = cv.xfeatures2d.SIFT_create()\n",
    "sift = cv.SIFT_create()\n",
    "\n",
    "kp = sift.detect(gray, None)\n",
    "# use detectAndCompute(...) to get descriptors themselves\n",
    "\n",
    "print(f\"Location ({kp[0].pt[0]:.2f}, {kp[0].pt[1]:.2f})\")\n",
    "print(f\"Radius: {kp[0].size};  angle:{kp[0].angle}\")\n",
    "img=cv.drawKeypoints(gray, kp, img, flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Discuss what you see here. What is the meaning of circle diameter? Of the angle?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index of keypoints\n",
    "\n",
    "Let's suppose we've found image descriptors. How do we find similar images, having this information? In our case the descriptors are 128-dinensional vectors per keypoint, and there can be hundreds of such points. To enable fast search of similar images, you will index descriptors of all images using some data structure for approximate nearest neighbors search, such as Navigable Small World or Annoy. Then, for a new (query) image you will generate descriptors, and for each of them find its nearest neighbors (using Euclidean or Cosine distance, which you prefer). Finally, you will sort potential similar images (retrieved from neighbor descriptors) by frequency with which they appear in the nearest neighbors (more matches -- higher the rank).\n",
    "\n",
    "### Build an index\n",
    "\n",
    "Read all images, saving category information. For every image generate SIFT descriptors and index them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all images and add their descriptors to index\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_sift_descriptors(img_path):    \n",
    "\n",
    "    #TODO return keypoints and their descriptors\n",
    "\n",
    "    return kp, des\n",
    "\n",
    "\n",
    "def read_dataset(img_dir):\n",
    "    for filename in glob.iglob(img_dir + '/*/*.jpg', recursive=True):\n",
    "        category = filename.split('/')[-2]\n",
    "        fn = \"/\".join(filename.split('/')[-2:])\n",
    "        kp, des = generate_sift_descriptors(filename)    \n",
    "        yield fn, kp, des, category\n",
    "        \n",
    "\n",
    "def get_top_descriptors(kp, des, top_k):\n",
    "    response_sort_indices = [i for (v, i) in sorted(((v, i) for (i, v) in enumerate(kp)), \n",
    "                                       key=lambda k: k[0].response, reverse=True)]        \n",
    "    top_des = np.take(des, response_sort_indices[:top_k], axis=0)\n",
    "    return top_des"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {}\n",
    "vectors = {}\n",
    "filenames = []\n",
    "\n",
    "for filename, keypoints, descriptors, category in tqdm(read_dataset(img_dir), total=9144):\n",
    "    categories[filename] = category\n",
    "    vectors[filename] = get_top_descriptors(keypoints, descriptors, 32)\n",
    "    filenames.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "lookup = []\n",
    "annoy = AnnoyIndex(128, 'euclidean')\n",
    "\n",
    "for filename in filenames:\n",
    "    for i, v in enumerate(vectors[filename]):\n",
    "        annoy.add_item(len(lookup), v)\n",
    "        lookup.append([filename, i])\n",
    "\n",
    "annoy.build(100, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement search function\n",
    "\n",
    "Implement a function which returns `k` neighbours (names) sorted for a given image name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def anns(imagename, k):\n",
    "    vecs = vectors[imagename]\n",
    "    # TODO\n",
    "    # return the list of ordered pairs, s\n",
    "    # imilarity first, better is in the beginning\n",
    "    return [(-1, imagename)]\n",
    "\n",
    "# finds query image in the result, as it is indexed\n",
    "filename = 'strawberry/image_0022.jpg'\n",
    "result = anns(filename, 10)\n",
    "assert any([f[1] == filename for f in result]), \"Should return a duplicate\"\n",
    "\n",
    "print(*result, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate the quality\n",
    "\n",
    "Build a bucket from these images.\n",
    "```\n",
    "accordion/image_0043.jpg\n",
    "laptop/image_0052.jpg\n",
    "pagoda/image_0038.jpg\n",
    "revolver/image_0043.jpg\n",
    "rhino/image_0040.jpg\n",
    "sea_horse/image_0038.jpg\n",
    "soccer_ball/image_0057.jpg\n",
    "starfish/image_0011.jpg\n",
    "strawberry/image_0022.jpg\n",
    "wrench/image_0013.jpg\n",
    "```\n",
    "Consider `relevant` if **class of the query and class of the result match**. Compute `DCG` for every query and for the bucket in average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep classifiers and Embeddings\n",
    "\n",
    "Based on:\n",
    "- https://www.analyticsvidhya.com/blog/2020/08/top-4-pre-trained-models-for-image-classification-with-python-code/\n",
    "- https://github.com/christiansafka/img2vec\n",
    "- https://github.com/ultralytics/yolov5\n",
    "\n",
    "### Obtain a single label for the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # or yolov5m, yolov5l, yolov5x, custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 'http://www.vision.caltech.edu/Image_Datasets/Caltech101/SamplePics/image_0022.jpg'\n",
    "results = model(i)\n",
    "pandas_detections_df = results.pandas().xyxy[0]\n",
    "pandas_detections_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the classes for the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "for filename in filenames[::400]:\n",
    "    results = model(img_dir + \"/\" + filename)\n",
    "    tag = results.pandas().xyxy[0]['name']\n",
    "    tag = tag[0] if len(tag) else None\n",
    "    cat = categories[filename]\n",
    "    print(f\"{filename:25}\\t{cat}\\t{tag}\")\n",
    "    plt.figure(figsize=(3,2))\n",
    "    plt.imshow(cv.imread(img_dir + \"/\" + filename)[:, :, ::-1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discuss:** \n",
    "- Look at the results. \n",
    "- Can we use this for retrieval in the same way as we used SIFT features? \n",
    "- What if the labels are different from original? What if there are multiple or no labels?\n",
    "\n",
    "## Vector embedding for image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install img2vec_pytorch Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from img2vec_pytorch import Img2Vec\n",
    "from PIL import Image\n",
    "\n",
    "# Initialize Img2Vec\n",
    "img2vec = Img2Vec(cuda=False)\n",
    "\n",
    "# Read in an image (rgb format)\n",
    "\n",
    "img_file = img_dir + '/gramophone/image_0018.jpg'\n",
    "img = Image.open(img_file)\n",
    "vector = img2vec.get_vec([img]).reshape(-1)\n",
    "vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX = 914 # 9144\n",
    "\n",
    "embedding_vectors = []\n",
    "borders = list(range(0, MAX, 100)) + [MAX]\n",
    "print(borders)\n",
    "\n",
    "def get_vectors(filenames):\n",
    "    # TODO\n",
    "    # return the np.array with the shape of (files x 512)\n",
    "    return ev\n",
    "\n",
    "for i in range(len(borders) - 1):\n",
    "    embedding_vectors += [get_vectors(filenames[borders[i]:borders[i+1]])]\n",
    "\n",
    "embedding_vectors = np.vstack(embedding_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "d = pairwise_distances(embedding_vectors, metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(d, cmap='RdBu', vmin=0, vmax=1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
