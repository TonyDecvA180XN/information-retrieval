{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language and Topic models\n",
    "\n",
    "Objectives:\n",
    "- visualizing unigram models together\n",
    "- implementing smoothing\n",
    "- ranking with probabilistics model\n",
    "- topic modelling with few lines of code\n",
    "\n",
    "A common suggestion to users for coming up with good queries is to think of words that would likely appear in a relevant document, and to use those words as the query. The language modeling approach to IR directly models this idea: a document is a good match to a query if the document model is likely to generate the query, which will in turn happen if the document contains the query words often. \n",
    "\n",
    "Today we will score documents with respect to user query using language models and also get some experience with topic modelling.\n",
    "\n",
    "## Loading the data\n",
    "\n",
    "We use the dataset we already used once - [this topic-modeling dataset](https://code.google.com/archive/p/topic-modeling-tool/downloads) ([or from github](https://github.com/IUCVLab/information-retrieval/blob/main/datasets/topic-modelling.zip))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def read_dataset(file_path):\n",
    "    docs = []\n",
    "    with open(file_path) as fp:\n",
    "        for cnt, line in enumerate(fp):\n",
    "            docs.append(line)\n",
    "    return docs\n",
    "\n",
    "folder = \"../datasets/topic-modelling\"\n",
    "\n",
    "fuel_data = read_dataset(os.path.join(folder, \"testdata_news_fuel_845docs.txt\"))\n",
    "brain_inj_data = read_dataset(os.path.join(folder, \"testdata_braininjury_10000docs.txt\"))\n",
    "economy_data = read_dataset(os.path.join(folder, \"testdata_news_economy_2073docs.txt\"))\n",
    "music_data = read_dataset(os.path.join(folder, \"testdata_news_music_2084docs.txt\"))\n",
    "\n",
    "all_data = fuel_data + brain_inj_data + economy_data + music_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ranges = [(\"fuel\", len(fuel_data)), \n",
    "               (\"brain_injury\", len(fuel_data)+len(brain_inj_data)),\n",
    "               (\"economy\", len(fuel_data)+len(brain_inj_data)+len(economy_data)), \n",
    "               (\"music\", len(all_data))]\n",
    "\n",
    "\n",
    "def get_doc_category(doc_id, ranges):\n",
    "    for r in ranges:\n",
    "        if doc_id < r[1]:\n",
    "            return r[0]\n",
    "    return \"out of range\"\n",
    "\n",
    "\n",
    "print(\"# of documents:\", len(all_data))\n",
    "assert len(all_data) == 15002\n",
    "print(\"data_ranges\", data_ranges)\n",
    "all_data[0][:300] + \"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking Using Language Models\n",
    "Our goal is to rank documents by $P(d|q)$, where the probability of a document is interpreted as the likelihood that it is relevant to the query. \n",
    "\n",
    "Using Bayes rule: $P(d|q) = \\frac{P(q|d)P(d)}{P(q)}$\n",
    "\n",
    "$P(q)$ is the same for all documents, and so can be ignored. The prior probability of a document $P(d)$ is often treated as uniform across all $d$'s and so it can also be ignored. What does it mean? \n",
    "\n",
    "It means that computing $P(q|d)$ for different documents we can compare how relevant are they to the query. How can we estimate $P(q|d)$?\n",
    "\n",
    "$P(q|d)$ can be estimated as:\n",
    "\n",
    "![](https://i.imgur.com/BEIMAC1.png)\n",
    "\n",
    "where $M_d$ is the language model of document $d$, $tf_{t,d}$ is the term frequency of term $t$ in document $d$, and $L_d$ is the number of tokens in document $d$. That is, we just count up how often each word occurred, and divide by the total number of words in the document $d$.\n",
    "\n",
    "### Build TDM (or DTM)\n",
    "\n",
    "The first thing we need to do is to build a term-document matrix for tour dataset. Use [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# TODO: build term-document matrix for the dataset\n",
    "# ...\n",
    "\n",
    "vectorizer =   # TODO vectorizer with stopwords\n",
    "counts_data =  # fit\n",
    "terms =        # names of dimensions\n",
    "\n",
    "print(\"vocabulary size\", len(terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the dataset in general, and it's topic parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "counts_total = counts_data.sum(axis=0).A1\n",
    "indices = np.argsort(counts_total)[::-1]\n",
    "print(np.array(terms)[indices][:20])\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "MAX = 100\n",
    "plt.plot( counts_total[indices][:MAX] / counts_total[indices[0]], label='all', lw=3, c='black')\n",
    "\n",
    "\n",
    "for i in range(len(data_ranges)):\n",
    "    low = data_ranges[i-1][1] if i > 0 else 0\n",
    "    high = data_ranges[i][1]\n",
    "    text = data_ranges[i][0]\n",
    "    counts = counts_data[low:high].sum(axis=0).A1\n",
    "    plt.plot(counts[indices][:MAX] / max(counts), label=text, ls='', markersize=7, marker=\"o\")\n",
    "\n",
    "plt.xlabel(\"words\")\n",
    "plt.ylabel(\"counts\")\n",
    "plt.xticks(range(MAX), np.array(terms)[indices[:MAX]], rotation=90)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: discuss the observations:\n",
    "- Why does the curve (line) look like this?\n",
    "- What does a circle below and beyond the line mean?\n",
    "- What does the biggest values mean? Why don't the are the same?\n",
    "- We discussed that there are such words in the language, which are almost everythere. Where they are?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing\n",
    "\n",
    "Now, you need to implement the abovementioned logic in the `lm_rank_documents` function below. Do you see any potential problems?\n",
    "\n",
    "Yes, data sparsity - we don't expect to meet each term in each doc, so, in most cases, we will get zero scores, which is not what we really want.\n",
    "\n",
    "The solution is smooting.\n",
    "\n",
    "One option is *[additive smoothing](https://en.wikipedia.org/wiki/Additive_smoothing)* - adding a small number (0 to 1) to the observed counts and renormalizing to give a probability distribution.\n",
    "\n",
    "Another option is called [Jelinek-Mercer smoothing](http://mlwiki.org/index.php/Smoothing_for_Language_Models#Jelinek-Mercer_Smoothing) - a simple idea that works well in practice is to use a mixture between a document-specific distribution and distribution estimated from the entire collection:\n",
    "\n",
    "![](https://i.imgur.com/8Qv41Wp.png)\n",
    "\n",
    "where $0 < Î» < 1$ and $M_c$ is a language model built from the entire document collection.\n",
    "\n",
    "Refer to [*Chapter 12*](https://nlp.stanford.edu/IR-book/html/htmledition/language-models-for-information-retrieval-1.html) for the detailed explanation.\n",
    "\n",
    "\n",
    "You are going to apply both in your `lm_rank_documents` function. This function takes TDM or DTM as an input, and ranks all documents \"building\" a language model for each document, returning relative probabilities of query being generated by a document as a document's score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def lm_rank_documents(query, tdm, terms_list, smoothing='additive', param=0.001):\n",
    "    # TODO: score each document in tdm using this document's language model\n",
    "    # implement two types of smoothing. Looks up term frequencies in tdm\n",
    "    # return document scores in a convenient form\n",
    "    # param is alpha for additive / lambda for jelinek-mercer\n",
    "    \"\"\"\n",
    "    :param query: dict, term:count            \n",
    "    :param tdm: term-document matrix\n",
    "    :param terms_list: vocabulary list\n",
    "    :param smoothing: which smoothing to apply, either 'additive' or 'jelinek-mercer'\n",
    "    :param param: alpha for additive / lambda for jelinek-mercer\n",
    "    :return: list of scores, list of doc_ids sorted by their scores \n",
    "    \"\"\"\n",
    "    n_docs = tdm.shape[0]\n",
    "    doc_lengths = tdm.sum(axis=1)\n",
    "    len_collection = np.sum(doc_lengths)\n",
    "    scores = np.zeros(n_docs)\n",
    "    for term in query.keys():\n",
    "        # check if term exists\n",
    "        if term in terms_list:\n",
    "            # get term's id\n",
    "            term_id = terms_list.index(term)\n",
    "        else:\n",
    "            continue\n",
    "        query_tf = query[term]\n",
    "        # calculate collection frequency of a term\n",
    "        collection_tf = np.sum(tdm[:, term_id])\n",
    "        for doc_id in range(n_docs):\n",
    "            doc_tf = tdm[doc_id, term_id]\n",
    "            # apply smoothing of any\n",
    "            if smoothing == 'additive':\n",
    "                doc_score_factor = (doc_tf + param) / (doc_lengths[doc_id] + param*len(terms_list))\n",
    "            elif smoothing == 'jelinek':\n",
    "                                \n",
    "                ###################### TODO ###############################################\n",
    "                # complete the formula here:\n",
    "                # lambda = param\n",
    "                # in-document probability is a \"doc tf\" / \"doc length\"\n",
    "                # in-language probability is a \"collection tf\" / \"collection length\"\n",
    "                ###########################################################################\n",
    "                \n",
    "                doc_score_factor = ...\n",
    "            else:\n",
    "                doc_score_factor = doc_tf / doc_lengths[doc_id]\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            doc_score_factor = doc_score_factor**query_tf\n",
    "\n",
    "            if doc_id not in scores:\n",
    "                scores[doc_id] = 1\n",
    "            # accumulate scores\n",
    "            scores[doc_id] *= doc_score_factor\n",
    "    # sort doc_ids by scores\n",
    "    sorted_doc_ids = np.argsort(-scores)\n",
    "    return scores, sorted_doc_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "Check if this type of ranking gives meaningful results. For each query output document `category`, `doc_id`, `score`, and the *beginning* of the document, as it is shown below. Analyze if categories and contents match the queries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, download\n",
    "import string\n",
    "\n",
    "download('stopwords')\n",
    "download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_query(raw_query):\n",
    "    # lower-case, remove punctuation and stopwords\n",
    "    stop_words = list(string.punctuation) + stopwords.words('english')\n",
    "    return Counter([i for i in word_tokenize(raw_query.lower()) if i not in stop_words])\n",
    "\n",
    "\n",
    "def process_query(raw_query, counts_data, terms, data_ranges):\n",
    "    # TODO process user query and print search results including document category, id, score, and some part of it\n",
    "    query = prepare_query(raw_query)\n",
    "    print(\"user query:\", '\\033[95m' + raw_query + '\\033[0m', \"~\", query.most_common())\n",
    "    doc_scores, doc_ids_sorted = lm_rank_documents(query, counts_data, terms, smoothing='jelinek', param=0.01)\n",
    "    print(\"\\nsearch results:\")\n",
    "    for i in range(5):\n",
    "        doc_id = doc_ids_sorted[i]\n",
    "        print(f'\\033[1m{get_doc_category(doc_id, data_ranges)}\\033[0m#{doc_id} score={doc_scores[doc_id]:.5f} | {all_data[doc_id][:100]}...')\n",
    "    \n",
    "\n",
    "user_queries = [\"piano concert\", \"symptoms of head trauma\", \"wall street journal\"]\n",
    "for q in user_queries:\n",
    "    process_query(q, counts_data, terms, data_ranges)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:**\n",
    "- Discuss results for the code. What do you see?\n",
    "- Discuss implementation of the methods. Can you do faster? Can we use vector indices? Graph-based indices?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling\n",
    "\n",
    "Now let's use *Latent Dirichlet Allocation* to identify topics in this collection and check if they match the original topics (fuel, economy, etc.). Go through the tutorial [here](https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0) and apply the ideas there to our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: apply LDA to our dataset and output the resulting categories \n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "# Helper function\n",
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        \n",
    "        \n",
    "# Tweak the two parameters below\n",
    "number_topics = 4\n",
    "number_words = 20\n",
    "# Create and fit the LDA model\n",
    "lda = ... # TODO your code here!!!\n",
    "\n",
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda, vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect to see something like this (if collapsed, click on 3 dots):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "```\n",
    "Topic #0:\n",
    "brain injury patients tbi traumatic study cerebral results severe group cognitive clinical pressure imaging following outcome control using children test\n",
    "\n",
    "Topic #1:\n",
    "new said york news atlanta like times year service time people undated just music journal constitution city says com years\n",
    "\n",
    "Topic #2:\n",
    "patients injury injuries trauma head study results traumatic brain treatment cases patient fractures years case outcome methods clinical tbi surgery\n",
    "\n",
    "Topic #3:\n",
    "said year bush percent new enron company president government people economy years million state companies states economic united time billion\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
